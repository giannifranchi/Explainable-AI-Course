{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e7bc07",
   "metadata": {},
   "source": [
    "# XAI-MVA Lab- 2h — Variance-Based Sensitivity / Sobol Attribution for an Audio Model\n",
    "\n",
    "**Goal.** Build a small PyTorch model that classifies *synthetic* audio signals from a time–frequency representation, then **interpret** the decision using **variance-based (Sobol) attribution** over spectrogram patches.\n",
    "\n",
    "> This lab is fully self-contained (no dataset download, except if you aim to try on an existing dataset afterwards).  \n",
    "> Get the `sobol_attribution_method` implementation  at https://github.com/fel-thomas, we will use it for the XAI part.\n",
    "\n",
    "---\n",
    "\n",
    "## Outline (indicative)\n",
    "1. Generate a synthetic audio mini-dataset (sines + noise + AM)   \n",
    "2. Features: log-magnitude spectrogram  \n",
    "3. Small CNN + training loop  \n",
    "4. Evaluation & typical errors \n",
    "5. **Sobol attribution** on the spectrogram: interpretation & `grid_size` \n",
    "6. Questions / mini-exercises \n",
    "\n",
    "---\n",
    "\n",
    "### Notation\n",
    "- $x(t)$: time-domain signal  \n",
    "- $X(f,\\tau)$: STFT (time–frequency)  \n",
    "- Network input: $\\log(1+|X|)$ (“image” in time–frequency)  \n",
    "- Attribution: importance per *patch* (grid) on that image\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1228d237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Optionnel) Installs si vous exécutez sur une machine vierge\n",
    "# !pip -q install --upgrade numpy scipy matplotlib\n",
    "# !pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "SEED = 225\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb07ccb7",
   "metadata": {},
   "source": [
    "## 1) Synthetic audio dataset generation \n",
    "\n",
    "We generate short waveforms (e.g., 0.5 s at 8 kHz) with:\n",
    "- either a **low sine** (class 0) *or* a **high sine** (class 1),\n",
    "- a bit of **noise**,\n",
    "- a mild **amplitude modulation (AM)** so the task is not “too easy”.\n",
    "\n",
    "### Questions (answer in the notebook)\n",
    "- **Q1.** Why is AM useful to avoid an overly trivial dataset?\n",
    "- **Q2.** What is the risk if we always generate exactly the same phase / the same amplitude?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403e92ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "TODO(1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Demo: plot a few waveforms\u001b[39;00m\n\u001b[1;32m     64\u001b[0m sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8000\u001b[39m\n\u001b[0;32m---> 65\u001b[0m xs, ys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[synth_sample(sr\u001b[38;5;241m=\u001b[39msr, dur\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)])\n\u001b[1;32m     67\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(xs, ys)):\n",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Demo: plot a few waveforms\u001b[39;00m\n\u001b[1;32m     64\u001b[0m sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8000\u001b[39m\n\u001b[0;32m---> 65\u001b[0m xs, ys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[43msynth_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdur\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m)])\n\u001b[1;32m     67\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(xs, ys)):\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36msynth_sample\u001b[0;34m(sr, dur, f_low, f_high, label, snr_db, am_depth, add_interferer)\u001b[0m\n\u001b[1;32m     24\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0.0\u001b[39m, dur, T, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# TODO(1): sample / set the label in {0,1}\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# label = ...\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO(1)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# TODO(2): choose the main frequency f0 based on the label\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# f0 = ...\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO(2)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: TODO(1)"
     ]
    }
   ],
   "source": [
    "# Exercise 1 — Synthetic audio generator (warm-up)\n",
    "# Fill the TODOs to generate a 1D waveform x (torch.Tensor of shape [T]) and a label in {0,1}.\n",
    "# Hints:\n",
    "# - Use torch.sin for pure tones\n",
    "# - Apply a simple amplitude modulation: (1 + am_depth * sin(2π f_am t))\n",
    "# - Add white Gaussian noise to match a target SNR in dB\n",
    "# - Optionally add an interferer tone (a second sine) at a random frequency\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def synth_sample(sr=8000, dur=0.5,\n",
    "                 f_low=220.0, f_high=880.0,\n",
    "                 label=None,\n",
    "                 snr_db=10.0,\n",
    "                 am_depth=0.3,\n",
    "                 add_interferer=True):\n",
    "    \"\"\"Return (x, label) where x is a torch.Tensor [T] and label in {0,1}.\n",
    "    Class 0: dominant component at f_low\n",
    "    Class 1: dominant component at f_high\n",
    "    \"\"\"\n",
    "    T = int(sr * dur)\n",
    "    t = torch.linspace(0.0, dur, T, dtype=torch.float32)\n",
    "\n",
    "    # TODO(1): sample / set the label in {0,1}\n",
    "    # label = ...\n",
    "    raise NotImplementedError(\"TODO(1)\")\n",
    "\n",
    "    # TODO(2): choose the main frequency f0 based on the label\n",
    "    # f0 = ...\n",
    "    raise NotImplementedError(\"TODO(2)\")\n",
    "\n",
    "    # TODO(3): generate the main sinusoid s0(t)\n",
    "    # s0 = ...\n",
    "    raise NotImplementedError(\"TODO(3)\")\n",
    "\n",
    "    # TODO(4): apply a simple amplitude modulation (AM)\n",
    "    # f_am = ... (e.g., 2 to 6 Hz)\n",
    "    # env = ...\n",
    "    # x = env * s0\n",
    "    raise NotImplementedError(\"TODO(4)\")\n",
    "\n",
    "    # Optional interferer (a second tone)\n",
    "    if add_interferer:\n",
    "        # TODO(5): add an interferer sine with random frequency in [300, 1500] Hz\n",
    "        # fi = ...\n",
    "        # x = x + 0.3 * torch.sin(2*pi*fi*t + phase)\n",
    "        raise NotImplementedError(\"TODO(5)\")\n",
    "\n",
    "    # Add noise for target SNR\n",
    "    # TODO(6): add white noise so that SNR(x_clean, noise) ≈ snr_db\n",
    "    # - estimate signal power: P_signal = mean(x^2)\n",
    "    # - noise power: P_noise = P_signal / (10^(snr_db/10))\n",
    "    # - noise = sqrt(P_noise) * randn_like(x)\n",
    "    # x_noisy = x + noise\n",
    "    raise NotImplementedError(\"TODO(6)\")\n",
    "\n",
    "    # Optional: normalize to roughly [-1, 1]\n",
    "    x = x_noisy / (x_noisy.abs().max() + 1e-8)\n",
    "    return x, int(label)\n",
    "\n",
    "# Demo: plot a few waveforms\n",
    "sr = 8000\n",
    "xs, ys = zip(*[synth_sample(sr=sr, dur=0.5) for _ in range(4)])\n",
    "\n",
    "plt.figure()\n",
    "for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "    plt.plot(x.numpy() + i * 2.2, label=f\"y={y}\")\n",
    "plt.title(\"Synthetic waveforms (vertically shifted)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f78c6d",
   "metadata": {},
   "source": [
    "## 2) Feature: log-magnitude spectrogram (STFT)\n",
    "\n",
    "We turn $x(t)$ into a time–frequency “image” using the STFT.\n",
    "\n",
    "### Questions\n",
    "- **Q3.** Why do we often use `log(1 + |X|)` rather than raw `|X|`?\n",
    "- **Q4.** What trade-off is introduced by the choice of `n_fft` / `hop_length`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545219aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 — STFT log-magnitude features\n",
    "# Implement a simple log-magnitude spectrogram:\n",
    "#   feat = log(1 + |STFT(x)|)\n",
    "# Return shape: [F, TT]\n",
    "\n",
    "import torch\n",
    "\n",
    "def stft_logmag(x, n_fft=256, hop_length=64, win_length=256):\n",
    "    \"\"\"Returns a tensor [F, TT] (frequency x time) in log-magnitude.\"\"\"\n",
    "    # TODO(1): create a Hann window (torch.hann_window)\n",
    "    # window = ...\n",
    "    raise NotImplementedError(\"TODO(1)\")\n",
    "\n",
    "    # TODO(2): compute the complex STFT (torch.stft, return_complex=True)\n",
    "    # X = ...\n",
    "    raise NotImplementedError(\"TODO(2)\")\n",
    "\n",
    "    # TODO(3): magnitude -> log1p\n",
    "    # mag = ...\n",
    "    # feat = ...\n",
    "    raise NotImplementedError(\"TODO(3)\")\n",
    "\n",
    "    return feat\n",
    "\n",
    "# Visualization\n",
    "x0, _ = synth_sample(sr=sr, dur=0.5, label=0)\n",
    "x1, _ = synth_sample(sr=sr, dur=0.5, label=1)\n",
    "\n",
    "feat0 = stft_logmag(x0)\n",
    "feat1 = stft_logmag(x1)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(feat0.numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.title(\"Log-magnitude spectrogram (class 0)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(feat1.numpy(), aspect=\"auto\", origin=\"lower\")\n",
    "plt.title(\"Log-magnitude spectrogram (class 1)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b661371",
   "metadata": {},
   "source": [
    "## 3) PyTorch Dataset + DataLoader \n",
    "\n",
    "We wrap waveforms and spectrograms into a `Dataset`, and use a `DataLoader` for mini-batches.\n",
    "\n",
    "Tip: in real audio tasks, you may precompute features or compute them on-the-fly depending on I/O constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaa027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 — Dataset + DataLoaders\n",
    "# Implement __getitem__ to return:\n",
    "#   x_feat: torch.Tensor of shape [1, F, TT]\n",
    "#   y: int in {0,1}\n",
    "# Optional: simple time-shift augmentation\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SyntheticAudioTFDataset(Dataset):\n",
    "    def __init__(self, n=2000, sr=8000, dur=0.5, n_fft=256, hop_length=64,\n",
    "                 snr_db_range=(0, 20), do_time_shift=False):\n",
    "        self.n = n\n",
    "        self.sr = sr\n",
    "        self.dur = dur\n",
    "        self.n_fft = n_fft\n",
    "        self.hop = hop_length\n",
    "        self.snr_db_range = snr_db_range\n",
    "        self.do_time_shift = do_time_shift\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO(1): sample an SNR in the range, and a label\n",
    "        # snr_db = ...\n",
    "        # label = ...\n",
    "        raise NotImplementedError(\"TODO(1)\")\n",
    "\n",
    "        # TODO(2): generate waveform\n",
    "        # x, y = synth_sample(...)\n",
    "        raise NotImplementedError(\"TODO(2)\")\n",
    "\n",
    "        # TODO(3): (optional) random circular time shift\n",
    "        # if self.do_time_shift: ...\n",
    "        raise NotImplementedError(\"TODO(3)\")\n",
    "\n",
    "        # TODO(4): compute log-magnitude STFT features\n",
    "        # feat = stft_logmag(...)\n",
    "        raise NotImplementedError(\"TODO(4)\")\n",
    "\n",
    "        # TODO(5): normalize features (e.g., per-sample mean/std)\n",
    "        # feat = (feat - feat.mean()) / (feat.std() + 1e-8)\n",
    "        raise NotImplementedError(\"TODO(5)\")\n",
    "\n",
    "        # TODO(6): add channel dimension -> [1, F, TT]\n",
    "        # feat = feat.unsqueeze(0)\n",
    "        raise NotImplementedError(\"TODO(6)\")\n",
    "\n",
    "        return feat, y\n",
    "\n",
    "# Build loaders\n",
    "train_ds = SyntheticAudioTFDataset(n=2000, do_time_shift=True)\n",
    "val_ds   = SyntheticAudioTFDataset(n=400,  do_time_shift=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# Quick sanity check\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch x:\", xb.shape, xb.dtype, \"Batch y:\", yb.shape, yb[:8].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3383395",
   "metadata": {},
   "source": [
    "## 4) A small CNN (on spectrograms)\n",
    "\n",
    "We keep the architecture compact so training is fast on CPU.\n",
    "\n",
    "### Questions\n",
    "- **Q5.** Why does a CNN naturally “see” local patterns on a spectrogram?\n",
    "- **Q6.** What is the difference between “time invariance” vs “frequency invariance” in this problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a63042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 — A small CNN for spectrogram classification\n",
    "# Implement a simple ConvNet that maps [B, 1, F, TT] -> logits [B, 2]\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SmallSpecCNN(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "        # TODO(1): define a small convolutional feature extractor (Conv2d/BN/ReLU/Pool)\n",
    "        # self.net = nn.Sequential(...)\n",
    "        raise NotImplementedError(\"TODO(1)\")\n",
    "\n",
    "        # TODO(2): define a final classifier head (e.g., AdaptiveAvgPool2d + Linear)\n",
    "        # self.head = ...\n",
    "        raise NotImplementedError(\"TODO(2)\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO(3): forward pass\n",
    "        # z = self.net(x)\n",
    "        # z = ...\n",
    "        # logits = self.head(z)\n",
    "        raise NotImplementedError(\"TODO(3)\")\n",
    "\n",
    "# Device + model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SmallSpecCNN().to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6125f",
   "metadata": {},
   "source": [
    "## 5) Training\n",
    "\n",
    "We train with cross-entropy and monitor validation loss/accuracy.\n",
    "\n",
    "**Tip:** with synthetic data, it is easy to accidentally “leak” information (e.g., fixed amplitudes, fixed noise, etc.). Keep an eye on generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 — Training loop\n",
    "# Fill the TODOs in train().\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    losses = []\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = ce(logits, y)\n",
    "            losses.append(loss.item())\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "    return float(np.mean(losses)), correct / total\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=6, lr=1e-3):\n",
    "    # TODO(1): create optimizer (Adam is fine)\n",
    "    # opt = ...\n",
    "    raise NotImplementedError(\"TODO(1)\")\n",
    "\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # TODO(2): forward pass -> loss\n",
    "            # logits = ...\n",
    "            # loss = ...\n",
    "            raise NotImplementedError(\"TODO(2)\")\n",
    "\n",
    "            # TODO(3): backward + optimizer step\n",
    "            # opt.zero_grad()\n",
    "            # loss.backward()\n",
    "            # opt.step()\n",
    "            raise NotImplementedError(\"TODO(3)\")\n",
    "\n",
    "            running_loss += loss.item() * y.numel()\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | train loss={train_loss:.3f}, acc={train_acc:.3f} | \"\n",
    "              f\"val loss={val_loss:.3f}, acc={val_acc:.3f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "history = train(model, train_loader, val_loader, epochs=6, lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a53022",
   "metadata": {},
   "source": [
    "## 6) Inspecting errors\n",
    "\n",
    "We inspect a few misclassified examples to understand what the model focuses on:\n",
    "- is it confusing classes when SNR is low?\n",
    "- is it over-relying on artifacts (noise, AM) rather than frequency content?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a21c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_misclassified(model, loader, max_items=8):\n",
    "    model.eval()\n",
    "    items=[]\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            bad = (pred != y).nonzero(as_tuple=False).squeeze(1)\n",
    "            for i in bad.tolist():\n",
    "                items.append((x[i].detach().cpu(), int(y[i].cpu()), int(pred[i].cpu())))\n",
    "                if len(items) >= max_items:\n",
    "                    return items\n",
    "    return items\n",
    "\n",
    "bad = collect_misclassified(model, val_loader, max_items=6)\n",
    "print(\"nb misclassified shown:\", len(bad))\n",
    "\n",
    "for i,(feat, y_true, y_pred) in enumerate(bad):\n",
    "    plt.figure()\n",
    "    plt.imshow(feat.squeeze(0).numpy(), origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(f\"Misclassified #{i} | true={y_true} pred={y_pred}\")\n",
    "    plt.xlabel(\"Temps\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934b5e2",
   "metadata": {},
   "source": [
    "## 7) Variance-based attribution (Sobol) on the spectrogram\n",
    "\n",
    "We want to measure the **importance of regions** of the spectrogram for the network’s decision.\n",
    "\n",
    "Idea:\n",
    "- split the input into a grid of patches (`grid_size`)\n",
    "- mask / replace patches according to binary variables\n",
    "- estimate Sobol indices (via Quasi Monte Carlo) to approximate each patch’s contribution to the output variance.\n",
    "\n",
    "### Things to explore\n",
    "- effect of `grid_size` (granularity)\n",
    "- effect of `nb_samples` (estimator variance)\n",
    "\n",
    "### Questions\n",
    "- **Q7.** Why does estimation become harder when `grid_size` increases?\n",
    "- **Q8.** On an audio spectrogram, which zones should matter to distinguish class 0 vs 1?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to import de la library (comme dans votre notebook d'exemple)\n",
    "has_sobol = False\n",
    "try:\n",
    "    from sobol_attribution_method.torch_explainer import SobolAttributionMethod\n",
    "    has_sobol = True\n",
    "except Exception as e:\n",
    "    print(\"SobolAttributionMethod not found :\", repr(e))\n",
    "has_sobol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fea11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attr(feat, attr, title=\"\"):\n",
    "    \"\"\"Affiche entrée + attribution (mêmes dimensions)\"\"\"\n",
    "    feat2 = feat.squeeze(0).numpy()\n",
    "    attr2 = attr.squeeze(0).numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(feat2, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(title + \" — spectrogram\")\n",
    "    plt.xlabel(\"Temps\")\n",
    "    plt.ylabel(\"Freq\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(attr2, origin=\"lower\", aspect=\"auto\")\n",
    "    plt.title(title + \" — attribution (Sobol)\")\n",
    "    plt.xlabel(\"Temps\")\n",
    "    plt.ylabel(\"Freq\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "if has_sobol:\n",
    "    model.eval()\n",
    "    # prendre un batch et choisir un exemple\n",
    "    xb, yb = next(iter(val_loader))\n",
    "    feat = xb[0].to(device)          # [1,F,T]\n",
    "    y = int(yb[0].item())\n",
    "    feat_b = feat.unsqueeze(0)       # [B=1,1,F,T]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(feat_b)\n",
    "        pred = int(logits.argmax(dim=1).item())\n",
    "    print(\"true:\", y, \"pred:\", pred)\n",
    "\n",
    "    # On construit un explainer. Plus nb_samples est grand, plus c'est long mais plus stable.\n",
    "    explainer = SobolAttributionMethod(model, grid_size=8, nb_samples=256, batch_size=32)\n",
    "\n",
    "    # L'API attend souvent une image (B,C,H,W) ; notre spectrogramme est déjà (B,1,F,T)\n",
    "    # On explique la probabilité de la classe prédite (ou la vraie classe).\n",
    "    target_class = pred\n",
    "    explanation = explainer.explain(feat_b, target=target_class)  # tensor (B,1,F,T) ou proche\n",
    "\n",
    "    show_attr(feat.cpu(), explanation[0].detach().cpu(), title=f\"Sobol grid=8 (target={target_class})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca4bbbb",
   "metadata": {},
   "source": [
    "## 8) Exploring `grid_size` and stability\n",
    "\n",
    "Try several `grid_size` values (e.g., 4, 8, 12) and compare:\n",
    "- spatial resolution of the attribution map,\n",
    "- stability across Monte Carlo runs.\n",
    "\n",
    "### Mini-exercise\n",
    "- **E5.** Fix `grid_size` and test `nb_samples ∈ {64, 256, 1024}`. What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd31a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7 — Sobol attribution (global variance-based explanation)\n",
    "# Fill the TODOs to compute Sobol attributions with different grid sizes.\n",
    "\n",
    "if has_sobol:\n",
    "    grid_sizes = [4, 8, 12]\n",
    "    nb_samples = 256\n",
    "\n",
    "    for gs in grid_sizes:\n",
    "        # TODO(1): instantiate the Sobol explainer\n",
    "        # explainer = SobolAttributionMethod(...)\n",
    "        raise NotImplementedError(\"TODO(1)\")\n",
    "\n",
    "        # TODO(2): run explanation for feat_b and a target class\n",
    "        # explanation = explainer.explain(...)\n",
    "        raise NotImplementedError(\"TODO(2)\")\n",
    "\n",
    "        show_attr(feat.cpu(), explanation[0].detach().cpu(),\n",
    "                  title=f\"Sobol grid={gs}, nb_samples={nb_samples}\")\n",
    "else:\n",
    "    print(\"Sobol library not installed — skip this section.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b14dc3",
   "metadata": {},
   "source": [
    "## 9) (Bonus) Compare with a gradient-based attribution\n",
    "\n",
    "We compute a simple saliency map (absolute gradient of the target logit w.r.t. the input).\n",
    "\n",
    "### Question\n",
    "- **Q9.** What does Sobol capture that saliency does not (and vice versa)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a45744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8 (bonus) — Gradient saliency for comparison\n",
    "# Implement a simple |d logit / d x| saliency map.\n",
    "\n",
    "def gradient_attribution(model, x, target_class):\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    logits = model(x)\n",
    "    loss = logits[0, target_class]\n",
    "    loss.backward()\n",
    "    return x.grad.abs().detach()\n",
    "\n",
    "# Get a batch (as you want ! val_loader is perhapse not the best ! )\n",
    "feat_b, y_b = next(iter(val_loader))\n",
    "feat_b = feat_b.to(device)\n",
    "y_b = y_b.to(device)\n",
    "\n",
    "# Pick one example\n",
    "x_demo = feat_b[0:1]\n",
    "y_demo = y_b[0].item()\n",
    "\n",
    "# Compute attribution\n",
    "attr_grad = gradient_attribution(model, x_demo, y_demo)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.imshow(attr_grad[0, 0].cpu(), origin=\"lower\", aspect=\"auto\")\n",
    "plt.title(\"Gradient-based attribution\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9f231",
   "metadata": {},
   "source": [
    "## 10) Wrap-up questions\n",
    "\n",
    "Answer briefly \n",
    "\n",
    "2. **Q10.** How would you choose `grid_size` in a principled way? Give 2 criteria.\n",
    "3. **Q11.** Give one potential failure case: an attribution map that looks “nice” but is misleading.\n",
    "4. **Q12.** What if your model exploits noise instead of frequency cues? How would you detect it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f0fc1",
   "metadata": {},
   "source": [
    "# 11) To go further and more interesting applications (part of the project in audio ! )\n",
    "1. try with a multi-class frequency (not only low and high frequency)\n",
    "2. Try other representations (ERB, Mel Spectrogram) and check the differences\n",
    "3. try with more realistic dataset and other architecture here is an non exhaustive list:\n",
    "- speech commands https://huggingface.co/datasets/google/speech_commands and AST https://huggingface.co/MIT/ast-finetuned-speech-commands-v2\n",
    "- music genre https://huggingface.co/datasets/ccmusic-database/music_genre  (classic VGG, VIT, Convnext pretrained models)\n",
    "4. provide a complete analysis using the same XAI system and previous code \n",
    "5. Other recommendations: you could also think preparing a small test dataset for pointing game or delation scores etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "variance_based",
   "language": "python",
   "name": "variance_based"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
